# -*- coding: utf-8 -*-
"""
Created on Wed May 28 10:51:37 2025

@author: Xintang Zheng

æ˜Ÿæ˜Ÿ: â˜… â˜† âœª âœ© ğŸŒŸ â­ âœ¨ ğŸŒ  ğŸ’« â­ï¸
å‹¾å‹¾å‰å‰: âœ“ âœ” âœ• âœ– âœ… â
æŠ¥è­¦å•¦: âš  â“˜ â„¹ â˜£
ç®­å¤´: â” âœ â™ â¤ â¥ â†© â†ª
emoji: ğŸ”” â³ â° ğŸ”’ ğŸ”“ ğŸ›‘ ğŸš« â— â“ âŒ â­• ğŸš€ ğŸ”¥ ğŸ’§ ğŸ’¡ ğŸµ ğŸ¶ ğŸ§­ ğŸ“… ğŸ¤” ğŸ§® ğŸ”¢ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ§  ğŸ“

"""
import os
import pandas as pd
from pathlib import Path
from typing import List, Dict, Union, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed
import glob
from functools import partial
from tqdm import tqdm


def read_parquet_files(folder_path: str, file_patterns: List[str] = None, 
                      suffix_list: List[str] = None) -> Dict[str, pd.DataFrame]:
    """
    è¯»å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„parquetæ–‡ä»¶
    
    Args:
        folder_path: æ–‡ä»¶å¤¹è·¯å¾„
        file_patterns: æ–‡ä»¶åæ¨¡å¼åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™è¯»å–æ‰€æœ‰.parquetæ–‡ä»¶
        suffix_list: åç¼€åˆ—è¡¨ï¼Œç”¨äºåŒ¹é…ç‰¹å®šåç¼€çš„æ–‡ä»¶
    
    Returns:
        æ–‡ä»¶ååˆ°DataFrameçš„æ˜ å°„å­—å…¸
    """
    folder_path = Path(folder_path)
    files_dict = {}
    
    if file_patterns is None:
        # è¯»å–æ‰€æœ‰parquetæ–‡ä»¶
        parquet_files = list(folder_path.glob('*.parquet'))
        for file_path in parquet_files:
            file_name = file_path.stem  # å»æ‰.parquetåç¼€
            files_dict[file_name] = pd.read_parquet(file_path)
    else:
        # æ ¹æ®æŒ‡å®šæ¨¡å¼è¯»å–æ–‡ä»¶
        for pattern in file_patterns:
            if isinstance(pattern, dict) and 'pos' in pattern and 'neg' in pattern:
                # å¤„ç†pos/negæ ¼å¼
                pos_name = pattern['pos']
                neg_name = pattern['neg']
                
                if suffix_list:
                    for suffix in suffix_list:
                        pos_files = list(folder_path.glob(f'{pos_name}*{suffix}.parquet'))
                        neg_files = list(folder_path.glob(f'{neg_name}*{suffix}.parquet'))
                        
                        if pos_files and neg_files:
                            pos_df = pd.read_parquet(pos_files[0])
                            neg_df = pd.read_parquet(neg_files[0])
                            key = f"{pos_name}_{neg_name}_{suffix}"
                            files_dict[key] = {
                                'pos_filter': pos_df,
                                'neg_filter': neg_df
                            }
                else:
                    # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶å¹¶æŒ‰åç¼€é…å¯¹
                    pos_files = list(folder_path.glob(f'{pos_name}*.parquet'))
                    neg_files = list(folder_path.glob(f'{neg_name}*.parquet'))
                    
                    # æå–åç¼€å¹¶é…å¯¹
                    pos_suffixes = {f.stem.replace(pos_name, '').lstrip('_'): f for f in pos_files}
                    neg_suffixes = {f.stem.replace(neg_name, '').lstrip('_'): f for f in neg_files}
                    
                    common_suffixes = set(pos_suffixes.keys()) & set(neg_suffixes.keys())
                    
                    for suffix in common_suffixes:
                        pos_df = pd.read_parquet(pos_suffixes[suffix])
                        neg_df = pd.read_parquet(neg_suffixes[suffix])
                        key = f"{pos_name}_{neg_name}_{suffix}" if suffix else f"{pos_name}_{neg_name}"
                        files_dict[key] = {
                            'pos_filter': pos_df,
                            'neg_filter': neg_df
                        }
            else:
                # å¤„ç†æ™®é€šå­—ç¬¦ä¸²æ¨¡å¼
                pattern_files = list(folder_path.glob(f'{pattern}*.parquet'))
                for file_path in pattern_files:
                    file_name = file_path.stem
                    files_dict[file_name] = pd.read_parquet(file_path)
    
    return files_dict


def create_filter_task(alpha_df: pd.DataFrame, filter_data: Union[pd.DataFrame, Dict], 
                      filter_func_name: str, filter_name: str, save_path: str) -> Dict:
    """
    åˆ›å»ºå•ä¸ªè¿‡æ»¤ä»»åŠ¡
    
    Args:
        alpha_df: åŸå§‹ä¿¡å·DataFrame
        filter_data: è¿‡æ»¤å› å­æ•°æ®
        filter_func_name: è¿‡æ»¤å‡½æ•°åç§°
        filter_name: è¿‡æ»¤å™¨åç§°
        save_path: ä¿å­˜è·¯å¾„
    
    Returns:
        ä»»åŠ¡å­—å…¸
    """
    return {
        'alpha_df': alpha_df,
        'filter_data': filter_data,
        'filter_func_name': filter_func_name,
        'filter_name': filter_name,
        'save_path': save_path
    }


def execute_filter_task(task: Dict, apply_filter_module_globals: Dict) -> bool:
    """
    æ‰§è¡Œå•ä¸ªè¿‡æ»¤ä»»åŠ¡
    
    Args:
        task: ä»»åŠ¡å­—å…¸
        apply_filter_module_globals: åŒ…å«apply_filterå‡½æ•°çš„globalså­—å…¸
    
    Returns:
        æ‰§è¡ŒæˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    try:
        alpha_df = task['alpha_df']
        filter_data = task['filter_data']
        filter_func_name = task['filter_func_name']
        filter_name = task['filter_name']
        save_path = task['save_path']
        
        # è·å–è¿‡æ»¤å‡½æ•°
        filter_func = apply_filter_module_globals.get(filter_func_name)
        if filter_func is None:
            print(f"Error: Function {filter_func_name} not found in globals")
            return False
        
        # æ‰§è¡Œè¿‡æ»¤
        filtered_result = filter_func(alpha_df, filter_data)
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ä¿å­˜ç»“æœ
        filtered_result.to_parquet(save_path)
        print(f"Successfully saved filtered result: {filter_name} -> {save_path}")
        
        return True
        
    except Exception as e:
        print(f"Error processing task {task.get('filter_name', 'unknown')}: {str(e)}")
        return False


def process_signal_filters(alpha_path: str, 
                          filter_configs: List[Dict], 
                          save_dir: str,
                          apply_filter_module_globals: Dict,
                          max_workers: Optional[int] = None) -> None:
    """
    æ‰¹é‡å¤„ç†ä¿¡å·è¿‡æ»¤ä»»åŠ¡
    
    Args:
        alpha_path: åŸå§‹ä¿¡å·æ–‡ä»¶è·¯å¾„
        filter_configs: è¿‡æ»¤é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
            - 'filter_path': è¿‡æ»¤å› å­æ–‡ä»¶å¤¹è·¯å¾„
            - 'filter_names': é€‰å–å› å­ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            - 'suffix_list': åç¼€åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºpos/negæ¨¡å¼ï¼‰
            - 'apply_filter_func': apply_filterå‡½æ•°åç§°
            - 'save_name': è¯¥è¿‡æ»¤çš„å‘½å
        save_dir: æœ€ç»ˆä¿å­˜ç›®å½•
        apply_filter_module_globals: åŒ…å«apply_filterå‡½æ•°çš„æ¨¡å—globals
        max_workers: æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
    
    Example:
        filter_configs = [
            {
                'filter_path': '/path/to/filter1',
                'filter_names': ['factor1', 'factor2'],  # å¯é€‰
                'apply_filter_func': 'basic_filter',
                'save_name': 'basic_filtering'
            },
            {
                'filter_path': '/path/to/filter2',
                'filter_names': [{'pos': 'pos_factor', 'neg': 'neg_factor'}],
                'suffix_list': ['1d', '5d'],  # å¯é€‰
                'apply_filter_func': 'conditional_mul_filter',
                'save_name': 'conditional_filtering'
            }
        ]
    """
    # è¯»å–åŸå§‹ä¿¡å·
    print(f"Loading alpha data from: {alpha_path}")
    alpha_df = pd.read_parquet(alpha_path)
    
    # åˆå§‹åŒ–æ‰€æœ‰ä»»åŠ¡
    all_tasks = []
    task_names = []  # ç”¨äºè¿›åº¦æ¡æ˜¾ç¤º
    
    for config in filter_configs:
        filter_path = config['filter_path']
        filter_names = config.get('filter_names', None)
        suffix_list = config.get('suffix_list', None)
        apply_filter_func = config['apply_filter_func']
        save_name = config['save_name']
        
        print(f"Processing filter config: {save_name}")
        
        # è¯»å–è¿‡æ»¤å› å­æ–‡ä»¶
        filter_files = read_parquet_files(filter_path, filter_names, suffix_list)
        
        # ä¸ºæ¯ä¸ªè¿‡æ»¤å› å­åˆ›å»ºä»»åŠ¡
        for filter_name, filter_data in filter_files.items():
            save_path = os.path.join(save_dir, save_name, f"{filter_name}.parquet")
            
            task = create_filter_task(
                alpha_df=alpha_df,
                filter_data=filter_data,
                filter_func_name=apply_filter_func,
                filter_name=filter_name,
                save_path=save_path
            )
            all_tasks.append(task)
            task_names.append(f"{save_name}_{filter_name}")
    
    print(f"Created {len(all_tasks)} filtering tasks")
    
    # è®¾ç½®å·¥ä½œè¿›ç¨‹æ•°
    if max_workers is None:
        max_workers = min(os.cpu_count() or 1, len(all_tasks))
    
    print(f"Starting concurrent processing with {max_workers} workers")
    
    # åˆ›å»ºéƒ¨åˆ†å‡½æ•°ï¼ŒåŒ…å«apply_filter_module_globals
    execute_task_with_globals = partial(execute_filter_task, 
                                       apply_filter_module_globals=apply_filter_module_globals)
    
    # ä½¿ç”¨concurrent.futuresæ‰§è¡Œä»»åŠ¡å¹¶ç›‘æ§è¿›åº¦
    results = []
    success_count = 0
    failed_tasks = []
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {
            executor.submit(execute_task_with_globals, task): (task, task_name) 
            for task, task_name in zip(all_tasks, task_names)
        }
        
        # ä½¿ç”¨tqdmç›‘æ§è¿›åº¦
        with tqdm(total=len(all_tasks), desc="Processing filters", 
                 bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]") as pbar:
            
            for future in as_completed(future_to_task):
                task, task_name = future_to_task[future]
                
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result:
                        success_count += 1
                        pbar.set_postfix({'Success': success_count, 'Failed': len(failed_tasks)})
                    else:
                        failed_tasks.append(task_name)
                        pbar.set_postfix({'Success': success_count, 'Failed': len(failed_tasks)})
                        
                except Exception as e:
                    failed_tasks.append(task_name)
                    results.append(False)
                    print(f"\nError in task {task_name}: {str(e)}")
                    pbar.set_postfix({'Success': success_count, 'Failed': len(failed_tasks)})
                
                pbar.update(1)
    
    # ç»Ÿè®¡ç»“æœ
    total_count = len(results)
    
    print(f"\nProcessing completed: {success_count}/{total_count} tasks successful")
    
    if failed_tasks:
        print(f"Warning: {len(failed_tasks)} tasks failed:")
        for failed_task in failed_tasks[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªå¤±è´¥çš„ä»»åŠ¡
            print(f"  - {failed_task}")
        if len(failed_tasks) > 10:
            print(f"  ... and {len(failed_tasks) - 10} more failed tasks")
    
    return {
        'total_tasks': total_count,
        'successful_tasks': success_count,
        'failed_tasks': failed_tasks
    }


# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
def example_usage():
    """
    ä½¿ç”¨ç¤ºä¾‹
    """
    # å‡è®¾ä½ å·²ç»ä»apply_filteræ¨¡å—å¯¼å…¥äº†æ‰€æœ‰å‡½æ•°
    # from your_apply_filter_module import *
    
    # é…ç½®è¿‡æ»¤å‚æ•°
    filter_configs = [
        {
            'filter_path': '/path/to/basic_filters',
            'filter_names': ['momentum', 'reversal'],  # å¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è¯»å–æ‰€æœ‰parquetæ–‡ä»¶
            'apply_filter_func': 'basic_filter',
            'save_name': 'basic_filtering'
        },
        {
            'filter_path': '/path/to/conditional_filters',
            'filter_names': [
                {'pos': 'bull_market', 'neg': 'bear_market'},
                {'pos': 'high_vol', 'neg': 'low_vol'}
            ],
            'suffix_list': ['1d', '5d', '20d'],
            'apply_filter_func': 'conditional_mul_filter',
            'save_name': 'conditional_filtering'
        }
    ]
    
    # æ‰§è¡Œè¿‡æ»¤å¤„ç†
    result = process_signal_filters(
        alpha_path='/path/to/original_alpha.parquet',
        filter_configs=filter_configs,
        save_dir='/path/to/filtered_results',
        apply_filter_module_globals=globals(),  # ä¼ å…¥å½“å‰æ¨¡å—çš„globals
        max_workers=4
    )
    
    print(f"Processing summary: {result}")
    
    # å¯ä»¥æ ¹æ®è¿”å›ç»“æœè¿›è¡Œåç»­å¤„ç†
    if result['failed_tasks']:
        print("Some tasks failed, consider rerunning or checking the logs")


if __name__ == "__main__":
    example_usage()