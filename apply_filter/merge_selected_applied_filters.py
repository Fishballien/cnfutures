# -*- coding: utf-8 -*-
"""
Created on Wed May 29 2025

@author: Xintang Zheng

æ˜Ÿæ˜Ÿ: â˜… â˜† âœª âœ© ğŸŒŸ â­ âœ¨ ğŸŒ  ğŸ’« â­ï¸
å‹¾å‹¾å‰å‰: âœ“ âœ” âœ• âœ– âœ… â
æŠ¥è­¦å•¦: âš  â“˜ â„¹ â˜£
ç®­å¤´: â” âœ â™ â¤ â¥ â†© â†ª
emoji: ğŸ”” â³ â° ğŸ”’ ğŸ”“ ğŸ›‘ ğŸš« â— â“ âŒ â­• ğŸš€ ğŸ”¥ ğŸ’§ ğŸ’¡ ğŸµ ğŸ¶ ğŸ§­ ğŸ“… ğŸ¤” ğŸ§® ğŸ”¢ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ§  ğŸ“

"""
import os
import sys
from pathlib import Path
import pandas as pd
import numpy as np
from tqdm import tqdm
from typing import Union, Dict, Any
import yaml
from functools import partial
import concurrent.futures


# %% add sys path
file_path = Path(__file__).resolve()
file_dir = file_path.parents[0]
project_dir = file_path.parents[1]
sys.path.append(str(project_dir))


# %%
from utils.dirutils import load_path_config
from utils.datautils import compute_dataframe_dict_average
from utils.timeutils import period_shortcut
from test_and_eval.factor_tester import FactorTesterByContinuous, FactorTesterByDiscrete
from data_processing.ts_trans import ts_normalize
from test_and_eval.factor_evaluation import eval_one_factor_one_period


def process_group_applied_filters(args):
    """
    å¤„ç†å•ä¸ªç»„çš„å‡½æ•°ï¼Œé€‚ç”¨äºå¹¶è¡Œæ‰§è¡Œï¼ˆè¿‡æ»¤åå› å­ç‰ˆæœ¬ï¼‰
    
    Args:
        args: åŒ…å«æ‰€éœ€å‚æ•°çš„å…ƒç»„
        
    Returns:
        tuple: (group_num, group_scaled) - ç»„å·å’Œç»„çš„æ ‡å‡†åŒ–åçš„å¹³å‡å› å­
    """
    group_num, group_info, group_normalization_func, factor_normalization_func, price_path, fstart = args
    
    price_data = pd.read_parquet(price_path)
    price_index = price_data.loc[fstart:].index
    group_factor_dict, group_weight_dict = {}, {}
    
    # å¤„ç†ç»„å†…æ¯ä¸ªå› å­
    for idx in group_info.index:
        root_dir = group_info.loc[idx, 'root_dir']
        factor_name = group_info.loc[idx, 'factor_name']
        direction = group_info.loc[idx, 'direction']
        
        # åŠ è½½å› å­æ–‡ä»¶
        fac_path = Path(root_dir) / f'{factor_name}.parquet'
        if not fac_path.exists():
            print(f"è­¦å‘Š: å› å­æ–‡ä»¶ä¸å­˜åœ¨: {fac_path}")
            continue
            
        fac = pd.read_parquet(fac_path)
        
        # ä½¿ç”¨å•å› å­æ ‡å‡†åŒ–å‡½æ•°
        scaled_fac = factor_normalization_func(fac)
        
        # å­˜å‚¨å› å­åŠå…¶æƒé‡
        group_factor_dict[idx] = (direction * scaled_fac).reindex(index=price_index).replace([-np.inf, np.inf], np.nan).fillna(0)
        group_weight_dict[idx] = 1
    
    # å¦‚æœç»„å†…æ²¡æœ‰æœ‰æ•ˆå› å­ï¼Œè¿”å›ç©ºDataFrame
    if not group_factor_dict:
        return group_num, pd.DataFrame(index=price_index)
    
    # è®¡ç®—ç»„å¹³å‡å€¼å¹¶ä½¿ç”¨ç»„æ ‡å‡†åŒ–å‡½æ•°æ ‡å‡†åŒ–
    group_avg = compute_dataframe_dict_average(group_factor_dict, group_weight_dict)
    group_scaled = group_normalization_func(group_avg).replace([-np.inf, np.inf], np.nan).fillna(0)
    return group_num, group_scaled


class AppliedFilterMerger:
    
    def __init__(self, merge_select_applied_filters_name: str, select_name: str, 
                 merge_name: str, test_eval_filtered_alpha_name: str, max_workers=None):
        """
        åˆå§‹åŒ–è¿‡æ»¤åå› å­åˆå¹¶å™¨
        
        Args:
            merge_select_applied_filters_name: åˆå¹¶é…ç½®åç§°
            select_name: é€‰æ‹©åç§°
            merge_name: åŸå§‹åˆå¹¶åç§°
            test_eval_filtered_alpha_name: æµ‹è¯•è¯„ä¼°é…ç½®åç§°
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
        """
        self.merge_select_applied_filters_name = merge_select_applied_filters_name
        self.select_name = select_name
        self.merge_name = merge_name
        self.test_eval_filtered_alpha_name = test_eval_filtered_alpha_name
        self.max_workers = max_workers
        
        # åŠ è½½è·¯å¾„é…ç½®
        self.path_config = load_path_config(project_dir)
        self.result_dir = Path(self.path_config['result'])
        self.param_dir = Path(self.path_config['param']) / 'merge_selected_applied_filters'
        
        # è®¾ç½®ç›®å½•è·¯å¾„
        self.select_dir = self.result_dir / 'select_applied_filters' / f'{merge_name}_{test_eval_filtered_alpha_name}_{select_name}'
        self.merged_dir = self.result_dir / 'merge_selected_applied_filters' / f'{merge_name}_{test_eval_filtered_alpha_name}_{select_name}_{merge_select_applied_filters_name}'
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        config_path = self.param_dir / f'{merge_select_applied_filters_name}.yaml'
        self.config = self._load_config(config_path)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.merged_dir.mkdir(parents=True, exist_ok=True)
        
        self._init_utils()
        
    def _load_config(self, config_path: Union[str, Path]) -> Dict[str, Any]:
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        """
        config_path = Path(config_path)
        if not config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        return config
    
    def _init_utils(self):
        # ä»é…ç½®ä¸­è¯»å–ä¸¤ä¸ªä¸åŒçš„é¢„å¤„ç†å‚æ•°
        group_preprocess_params = self.config['group_preprocess_params']
        factor_preprocess_params = self.config['factor_preprocess_params']
        
        # åˆå§‹åŒ–ä¸¤ä¸ªä¸åŒçš„æ ‡å‡†åŒ–å‡½æ•°
        self.group_normalization_func = partial(ts_normalize, param=group_preprocess_params)
        self.factor_normalization_func = partial(ts_normalize, param=factor_preprocess_params)
        
    def run_one_period(self, date_start, date_end, eval_date_start=None, eval_date_end=None):
        """
        è¿è¡Œå•ä¸ªæœŸé—´çš„åˆå¹¶æ“ä½œ
        
        Args:
            date_start: å¼€å§‹æ—¥æœŸ
            date_end: ç»“æŸæ—¥æœŸ
            eval_date_start: è¯„ä¼°å¼€å§‹æ—¥æœŸï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨date_start
            eval_date_end: è¯„ä¼°ç»“æŸæ—¥æœŸï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨date_end
        """
        # ç”ŸæˆæœŸé—´åç§°
        period_name = period_shortcut(date_start, date_end)
        
        if eval_date_start is None:
            eval_date_start = date_start
        if eval_date_end is None:
            eval_date_end = date_end
        
        # è®¾ç½®æ­¤æœŸé—´çš„ç›®å½•
        self.select_period_dir = self.select_dir / period_name
        self.merged_period_dir = self.merged_dir / period_name
        self.merged_period_dir.mkdir(parents=True, exist_ok=True)
        
        self._merge_one_period(period_name)
        self._test_predicted(period_name)
        self._eval_predicted(eval_date_start, eval_date_end, period_name)
    
    def _merge_one_period(self, period_name):
        """
        åˆå¹¶æŒ‡å®šæœŸé—´å†…çš„è¿‡æ»¤åå› å­
        """
        select_period_dir = self.select_period_dir
        merged_period_dir = self.merged_period_dir
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨
        output_path = merged_period_dir / f'avg_predict_{period_name}.parquet'
        
        # æœ€ç»ˆé€‰å®šå› å­çš„è·¯å¾„
        final_selected_factors_path = select_period_dir / 'final_selected_factors.csv'
        
        # æ£€æŸ¥æœ€ç»ˆé€‰å®šçš„å› å­æ˜¯å¦å­˜åœ¨
        if not final_selected_factors_path.exists():
            print(f"æœªæ‰¾åˆ°æœŸé—´ {period_name} çš„æœ€ç»ˆé€‰å®šè¿‡æ»¤åå› å­")
            return None
        
        # åŠ è½½æœ€ç»ˆé€‰å®šçš„å› å­
        final_selected_factors = pd.read_csv(final_selected_factors_path)
        
        if len(final_selected_factors) == 0:
            print(f"æœŸé—´ {period_name} æ²¡æœ‰é€‰å®šçš„è¿‡æ»¤åå› å­")
            return None
        
        # æŒ‰ç»„åˆ†ç»„å› å­
        grouped = final_selected_factors.groupby('group')
        factor_dict, weight_dict = self._process_groups_parallel(grouped, period_name, max_workers=self.max_workers)
        
        if not factor_dict:
            print(f"æœŸé—´ {period_name} æ²¡æœ‰æœ‰æ•ˆçš„å› å­ç»„")
            return None
        
        # è®¡ç®—è·¨ç»„çš„æ€»ä½“å¹³å‡å€¼
        factor_avg = compute_dataframe_dict_average(factor_dict, weight_dict)
        # ä½¿ç”¨ç»„æ ‡å‡†åŒ–å‡½æ•°æ¥æ ‡å‡†åŒ–æœ€ç»ˆç»“æœ
        factor_scaled = self.group_normalization_func(factor_avg).replace([-np.inf, np.inf], np.nan).fillna(0)
        
        # ä¿å­˜ç»“æœ
        factor_scaled.to_parquet(output_path)
        
        print(f"åˆå¹¶è¿‡æ»¤åå› å­å·²ä¿å­˜è‡³ {output_path}")
        
    def _process_groups_parallel(self, grouped, period_name, max_workers=None):
        """
        å¹¶è¡Œå¤„ç†æ‰€æœ‰ç»„
        """
        price_path = self.config['price_path']
        fstart = self.config['fstart']
        factor_dict, weight_dict = {}, {}
        
        # å‡†å¤‡å¹¶è¡Œå¤„ç†çš„å‚æ•°
        group_args = [(group_num, group_info, self.group_normalization_func, 
                       self.factor_normalization_func, price_path, fstart) 
                      for group_num, group_info in grouped]
        total_groups = len(group_args)
        
        if max_workers == 1 or max_workers is None:
            # å•è¿›ç¨‹é¡ºåºå¤„ç†
            with tqdm(total=total_groups, desc=f'å¤„ç† {period_name} çš„è¿‡æ»¤åå› å­Groups [Single]') as pbar:
                for args in group_args:
                    group_num = args[0]
                    try:
                        group_num, group_avg = process_group_applied_filters(args)
                        if not group_avg.empty:
                            factor_dict[group_num] = group_avg
                            weight_dict[group_num] = 1
                    except Exception as exc:
                        print(f'å¤„ç†è¿‡æ»¤åå› å­ç»„ {group_num} æ—¶å‘ç”Ÿé”™è¯¯: {exc}')
                    finally:
                        pbar.update(1)
        else:
            # å¤šè¿›ç¨‹å¤„ç†
            with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_params = {executor.submit(process_test_info_applied_filters, *params): params for params in input_params}
                
                # ä½¿ç”¨as_completedè·å–å®Œæˆçš„ä»»åŠ¡å¹¶æ˜¾ç¤ºè¿›åº¦
                for future in tqdm(concurrent.futures.as_completed(future_to_params), total=total_tasks, desc="Processing tests"):
                    try:
                        res_dict = future.result()
                        res_list.append(res_dict)
                    except Exception as exc:
                        test_info = future_to_params[future][0]
                        print(f'Test {test_info["test_name"]} generated an exception: {exc}')
        
        # è½¬æ¢ä¸ºDataFrame
        res_df = pd.DataFrame(res_list)
        res_df.to_csv(merged_period_dir / 'evaluation.csv', index=None)


def process_test_info_applied_filters(test_info, factor_name, date_start, date_end, merged_period_dir, eval_param, price_path):
    """
    å¤„ç†æµ‹è¯•ä¿¡æ¯çš„ç‹¬ç«‹å‡½æ•°ï¼ˆè¿‡æ»¤åå› å­ç‰ˆæœ¬ï¼‰
    """
    mode = test_info['mode']
    test_name = test_info['test_name']
    eval_inputs = {
        "factor_name": factor_name,
        "date_start": date_start,
        "date_end": date_end,
        "data_date_start": date_start,
        "data_date_end": date_end,
        "process_name": '',
        "test_name": test_name,
        "tag_name": '',
        "data_dir": merged_period_dir / 'test' / test_name / 'data',
        "processed_data_dir": merged_period_dir,
        "valid_prop_thresh": eval_param['valid_prop_thresh'],
        "fee": eval_param['fee'],
        "price_data_path": price_path,
        "mode": mode, 
    }
    
    return eval_one_factor_one_period(**eval_inputs)


def example_usage():
    """
    ä½¿ç”¨ç¤ºä¾‹
    """
    # åˆå§‹åŒ–AppliedFilterMerger
    afm = AppliedFilterMerger(
        merge_select_applied_filters_name='basic_merge',
        select_name='basic_select',
        merge_name='merge_v1',
        test_eval_filtered_alpha_name='basic_test_eval',
        max_workers=4
    )
    
    # å¯¹å•ä¸ªæœŸé—´è¿›è¡Œåˆå¹¶
    afm.run_one_period('20240101', '20240331')
    
    print("è¿‡æ»¤åå› å­åˆå¹¶å®Œæˆ")


if __name__ == "__main__":
    example_usage()