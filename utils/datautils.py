# -*- coding: utf-8 -*-
"""
Created on Thu Aug  1 16:23:13 2024

@author: Xintang Zheng

æ˜Ÿæ˜Ÿ: â˜… â˜† âœª âœ© ğŸŒŸ â­ âœ¨ ğŸŒ  ğŸ’« â­ï¸
å‹¾å‹¾å‰å‰: âœ“ âœ” âœ• âœ– âœ… â
æŠ¥è­¦å•¦: âš  â“˜ â„¹ â˜£
ç®­å¤´: â” âœ â™ â¤ â¥ â†© â†ª
emoji: ğŸ”” â³ â° ğŸ”’ ğŸ”“ ğŸ›‘ ğŸš« â— â“ âŒ â­• ğŸš€ ğŸ”¥ ğŸ’§ ğŸ’¡ ğŸµ ğŸ¶ ğŸ§­ ğŸ“… ğŸ¤” ğŸ§® ğŸ”¢ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ§  ğŸ“

"""
# %% imports
import pandas as pd
from pathlib import Path
import traceback
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from tqdm import tqdm
import numpy as np


from utils.speedutils import gc_collect_after
from utils.market import index_to_futures


# %% align
def align_columns(main_col, sub_df):
    sub_aligned = pd.DataFrame(columns=main_col, index=sub_df.index)
    inner_columns = main_col.intersection(sub_df.columns)
    sub_aligned[inner_columns] = sub_df[inner_columns]
    return sub_aligned


def align_index(df_1, df_2):
    inner_index = df_1.index.intersection(df_2.index)
    return df_1.loc[inner_index, :], df_2.loc[inner_index, :]


def align_index_with_main(main_index, sub_df):
    sub_aligned = pd.DataFrame(columns=sub_df.columns, index=main_index)
    inner_index = main_index.intersection(sub_df.index)
    sub_aligned.loc[inner_index, :] = sub_df.loc[inner_index, :]
    return sub_aligned


def align_to_primary(df_primary, df_secondary, key_column1, key_column2):
    # æ£€æŸ¥ä¸» DataFrame æ˜¯å¦æœ‰é‡å¤å€¼
    assert not df_primary.duplicated(subset=[key_column1, key_column2]).any(), "df_primary contains duplicate rows based on key_column1 and key_column2"
    
    # å–ä¸» DataFrame çš„é”®å€¼ç»„åˆ
    primary_keys = df_primary[[key_column1, key_column2]]
    
    # æ ¹æ®ä¸» DataFrame çš„é”®å€¼ç»„åˆé‡æ–°ç´¢å¼•æ¬¡è¦ DataFrame
    df_secondary_aligned = df_secondary.set_index([key_column1, key_column2]).reindex(pd.MultiIndex.from_frame(primary_keys)).reset_index()
    
    return df_secondary_aligned


def align_to_primary_by_col_list(df_primary, df_secondary, key_columns):
    # æ£€æŸ¥ä¸» DataFrame æ˜¯å¦æœ‰é‡å¤å€¼
    assert not df_primary.duplicated(subset=key_columns).any(), f"df_primary contains duplicate rows based on {key_columns}"
    
    # å–ä¸» DataFrame çš„é”®å€¼ç»„åˆ
    primary_keys = df_primary[key_columns]
    
    # æ ¹æ®ä¸» DataFrame çš„é”®å€¼ç»„åˆé‡æ–°ç´¢å¼•æ¬¡è¦ DataFrame
    df_secondary_aligned = (
        df_secondary
        .set_index(key_columns)
        .reindex(pd.MultiIndex.from_frame(primary_keys))
        .reset_index()
    )
    
    return df_secondary_aligned


def align_and_sort_columns(df_list):
    """
    å¯¹é½å¤šä¸ª DataFrame çš„å…±åŒåˆ—ï¼Œå¹¶æŒ‰åˆ—åå­—æ¯é¡ºåºé‡æ–°æ’åˆ—ã€‚

    å‚æ•°:
        df_list (list of pd.DataFrame): åŒ…å«å¤šä¸ª DataFrame çš„åˆ—è¡¨ã€‚

    è¿”å›:
        list of pd.DataFrame: å¯¹é½å¹¶é‡æ–°æ’åˆ—åˆ—é¡ºåºåçš„ DataFrame åˆ—è¡¨ã€‚
    """
    # æ‰¾å‡ºæ‰€æœ‰ DataFrame çš„å…±åŒåˆ—
    common_cols = sorted(set.intersection(*(set(df.columns) for df in df_list)))
    
    # æŒ‰å…±åŒåˆ—é‡æ–°ç´¢å¼•æ¯ä¸ª DataFrame
    aligned_dfs = [df[common_cols] for df in df_list]
    
    return aligned_dfs


# %% load one
@gc_collect_after
def get_one_factor(process_name=None, factor_name=None, factor_data_dir=None, 
                   price_type='future_twap', normalization_func=None, 
                   date_start=None, date_end=None, 
                   ref_order_col=None, ref_index=None, fix_changed_root=False, debug=0):
    if fix_changed_root:
        str_dir = str(factor_data_dir)
        if str_dir.endswith('neu'):
            factor_data_dir = Path(str_dir + '_4h')
    if debug:
        breakpoint()
    # process_name = 'ma15_sp240'
    factor_dir = factor_data_dir / process_name
    factor_path = factor_dir / f'{factor_name}.parquet'
    try:
        factor = pd.read_parquet(factor_path)
    except:
        print(factor_path)
        traceback.print_exc()
    if price_type in ['future', 'future_twap']:
        factor = factor.rename(columns=index_to_futures)
    factor = normalization_func(factor)
    factor = factor[(factor.index >= date_start) & (factor.index < date_end)]
    if ref_order_col is not None:
        factor = align_columns(ref_order_col, factor)
    if ref_index is not None:
        factor = align_index_with_main(ref_index, factor)
    return factor.fillna(0)


# %% load all & group
def load_all_factors(cluster_info, get_one_factor_func, factor_data_dir, n_workers):
    tasks = []
    
    for index in cluster_info.index:
        process_name, factor_name = cluster_info.loc[index, ['process_name', 'factor']]
        try:
            # å°è¯•è·å–ç‰¹å®šè¡Œå’Œåˆ—çš„å€¼
            factor_dir = Path(cluster_info.loc[index, 'root_dir'])
        except KeyError:
            # å¦‚æœåˆ—ä¸å­˜åœ¨ï¼Œè¿”å›é»˜è®¤å€¼
            factor_dir = factor_data_dir
        tasks.append((index, process_name, factor_name, factor_dir))
        
    factor_dict = {}
    if n_workers is None or n_workers == 1:
        for (index, pool_name, feature_name) in tasks:
            factor_dict[index] = get_one_factor_func(pool_name, feature_name, process_name, factor_name)
    else:
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = {executor.submit(get_one_factor_func, *task[1:-1], factor_data_dir=task[-1]): 
                       task[0] for task in tasks}

            for future in tqdm(as_completed(futures), total=len(futures), desc='load all factors ğŸ“Š'):
                idx_to_save = futures[future]
                factor = future.result()
                factor_dict[idx_to_save] = factor
    return factor_dict
                
                
# @gc_collect_after # TODOï¼šä¹‹åè¦æ”¹åŠ å…¥root dir
def load_one_group(group_num, group_info, factor_dict={}, normalization_func=None, to_mask=None):
    try:
        # group_name = f'group_{int(group_num)}'
        len_of_group = len(group_info)
        group_factor = None
        for id_, index in enumerate(group_info.index):
            process_name, factor_name, direction = group_info.loc[index, ['process_name', 'factor', 'direction']]
            factor = factor_dict[index]
            # factor = normalization_func(factor).fillna(0)
            if factor is None:
                len_of_group -= 1
                print(process_name, factor_name)
                continue
            factor = factor * direction
            if group_factor is None:
                group_factor = factor
            else:
                group_factor += factor
        group_factor = group_factor / len_of_group
    except:
        traceback.print_exc()
    return int(group_num), group_factor


# %% merge
def add_dataframe_to_dataframe_reindex(df, new_data):
    """
    ä½¿ç”¨ reindex å°†æ–° DataFrame çš„æ•°æ®æ·»åŠ åˆ°ç›®æ ‡ DataFrame ä¸­ï¼Œæ”¯æŒåŠ¨æ€æ‰©å±•åˆ—å’Œè¡Œï¼ŒåŸå…ˆæ²¡æœ‰å€¼çš„åœ°æ–¹å¡«å…… NaNã€‚

    å‚æ•°:
    df (pd.DataFrame): ç›®æ ‡ DataFrameã€‚
    new_data (pd.DataFrame): è¦æ·»åŠ çš„æ–° DataFrameã€‚

    è¿”å›å€¼:
    df (pd.DataFrame): æ›´æ–°åçš„ DataFrameã€‚
    """
    # åŒæ—¶æ‰©å±•è¡Œå’Œåˆ—ï¼Œå¹¶ç¡®ä¿æœªå¡«å……çš„ç©ºå€¼ä¸º NaNï¼ŒæŒ‰æ’åº
    df = df.reindex(index=df.index.union(new_data.index, sort=True),
                    columns=df.columns.union(new_data.columns, sort=True),
                    fill_value=np.nan)
    
    # ä½¿ç”¨ loc æ·»åŠ æ–°æ•°æ®
    df.loc[new_data.index, new_data.columns] = new_data

    return df


# %% average
# def compute_dataframe_dict_average(df_dict):
#     """
#     è®¡ç®— df_dict ä¸­æ¯ä¸ª DataFrame å¯¹åº”åˆ—çš„å¹³å‡å€¼ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«å¹³å‡å€¼çš„ DataFrameã€‚
#     å¯¹äºæ¯ä¸ª DataFrameï¼Œå…ˆæŒ‰ç¬¬ä¸€ä¸ª DataFrame çš„ index å’Œ columns è¿›è¡Œ reindexï¼Œå†æ±‚å¹³å‡å€¼ã€‚
#     å¦‚æœæŸä¸ªä½ç½®çš„å€¼ä¸º NaNï¼Œåˆ™è¯¥ä½ç½®çš„å¹³å‡å€¼ä¹Ÿä¸º NaNã€‚
    
#     å‚æ•°:
#     df_dict (dict): å­—å…¸ï¼Œå…¶ä¸­çš„æ¯ä¸ªå€¼æ˜¯ä¸€ä¸ª DataFrameã€‚
    
#     è¿”å›:
#     pd.DataFrame: åŒ…å«å„åˆ—å¹³å‡å€¼çš„ DataFrameï¼Œindex ä¿æŒåŸ DataFrame çš„ indexã€‚
#     """
#     # è·å–ç¬¬ä¸€ä¸ª DataFrame çš„åˆ—åå’Œ index
#     first_df = list(df_dict.values())[0]
#     columns = first_df.columns
#     index = first_df.index

#     # åˆå§‹åŒ–å­˜å‚¨å¹³å‡å€¼çš„å­—å…¸
#     averages = {}

#     # å¯¹æ¯ä¸€åˆ—è¿›è¡Œè®¡ç®—
#     for col in columns:
#         # å¯¹æ¯ä¸ª DataFrame è¿›è¡Œ reindexï¼Œç¡®ä¿å®ƒä»¬çš„ index å’Œ columns ä¸ç¬¬ä¸€ä¸ª DataFrame å¯¹é½
#         reindexed_dfs = [df.reindex(index=index, columns=columns) for df in df_dict.values()]

#         # å¯¹äºæ¯ä¸ªä½ç½®çš„åˆ—å€¼æ±‚å’Œï¼Œä½¿ç”¨ np.sum å¿½ç•¥ NaN
#         sum_values = np.sum([df[col].values for df in reindexed_dfs], axis=0)

#         # æ£€æŸ¥æ¯ä¸ªä½ç½®æ˜¯å¦æœ‰ NaNï¼Œè‹¥æœ‰ NaNï¼Œåˆ™è¯¥ä½ç½®çš„å¹³å‡å€¼ä¸º NaN
#         is_nan = np.any([np.isnan(df[col].values) for df in reindexed_dfs], axis=0)

#         # å¦‚æœæœ‰ NaNï¼Œåˆ™å¯¹åº”ä½ç½®çš„å¹³å‡å€¼ä¸º NaN
#         averages[col] = np.where(is_nan, np.nan, sum_values / len(df_dict))

#     # å°†ç»“æœè½¬æ¢ä¸ºä¸€ä¸ª DataFrameï¼Œè®¾ç½® index ä¸ºç¬¬ä¸€ä¸ª df çš„ index
#     average_df = pd.DataFrame(averages, index=index)
    
#     return average_df


def compute_dataframe_dict_average(df_dict, weight_dict):
    """
    è®¡ç®— df_dict ä¸­æ¯ä¸ª DataFrame å¯¹åº”åˆ—çš„åŠ æƒå¹³å‡å€¼ï¼Œè¿”å›ä¸€ä¸ªåŒ…å«åŠ æƒå¹³å‡å€¼çš„ DataFrameã€‚
    ä½¿ç”¨ df_dict çš„æ¯ä¸ª DataFrame çš„ index å’Œ columns çš„äº¤é›†ç”Ÿæˆæœ€ç»ˆçš„ç»“æœï¼Œ
    å¹¶ä½¿ç”¨ç»™å®šçš„ weight_dict å¯¹å„ DataFrame è¿›è¡ŒåŠ æƒå¹³å‡ã€‚
    
    å‚æ•°:
    df_dict (dict): å­—å…¸ï¼Œå…¶ä¸­çš„æ¯ä¸ªå€¼æ˜¯ä¸€ä¸ª DataFrameã€‚
    weight_dict (dict): å­—å…¸ï¼Œå…¶ä¸­çš„æ¯ä¸ªå€¼æ˜¯å¯¹åº” DataFrame çš„æƒé‡ã€‚
    
    è¿”å›:
    pd.DataFrame: åŒ…å«åŠ æƒå¹³å‡å€¼çš„ DataFrameï¼Œindex å’Œ columns æ˜¯ df_dict çš„äº¤é›†ã€‚
    """
    # è·å–æ‰€æœ‰ DataFrame çš„ index å’Œ columns çš„äº¤é›†
    common_index = df_dict[list(df_dict.keys())[0]].index
    common_columns = df_dict[list(df_dict.keys())[0]].columns

    for df in df_dict.values():
        common_index = common_index.intersection(df.index)
        common_columns = common_columns.intersection(df.columns)

    # å½’ä¸€åŒ–æƒé‡ï¼Œä½¿æ€»å’Œä¸º 1
    total_weight = sum(weight_dict.values())
    normalized_weights = {key: weight_dict[key] / total_weight for key in weight_dict}

    # åˆå§‹åŒ–å­˜å‚¨åŠ æƒå¹³å‡å€¼çš„å­—å…¸
    weighted_averages = {}

    # å¯¹æ¯ä¸€åˆ—è¿›è¡Œè®¡ç®—
    for col in common_columns:
        weighted_sum = np.zeros(len(common_index))  # å­˜å‚¨åŠ æƒå’Œ
        total_weight_for_column = 0  # ç”¨äºè¿½è¸ªæƒé‡æ€»å’Œï¼ˆå¯ä»¥ç”¨äºè°ƒè¯•ï¼‰

        # å¯¹æ¯ä¸ª DataFrame æŒ‰ç…§æƒé‡åŠ æƒè®¡ç®—
        for key, df in df_dict.items():
            # ç¡®ä¿è¯¥ DataFrame æœ‰è¿™ä¸ªåˆ—ï¼Œå¹¶ä¸”ç´¢å¼•ä¹Ÿåœ¨äº¤é›†ä¸­
            if col in df.columns:
                # å…ˆå¯¹è¯¥ DataFrame è¿›è¡Œ reindexï¼Œä½¿å…¶ç´¢å¼•ä¸äº¤é›†ä¸€è‡´
                reindexed_df = df.reindex(index=common_index, columns=common_columns)
                
                # è·å–è¯¥åˆ—çš„åŠ æƒå€¼ï¼Œå¿½ç•¥ NaN
                col_values = reindexed_df[col].values
                weight = normalized_weights.get(key, 0)
                weighted_sum += weight * col_values
                
                # æ›´æ–°æƒé‡æ€»å’Œ
                total_weight_for_column += weight
        
        # è®¡ç®—åŠ æƒå¹³å‡å€¼ï¼ˆå¦‚æœæœ‰ä»»ä½• NaNï¼Œåˆ™åŠ æƒå¹³å‡ä¹Ÿä¸º NaNï¼‰
        weighted_averages[col] = np.where(np.isnan(weighted_sum), np.nan, weighted_sum)

    # å°†ç»“æœè½¬æ¢ä¸ºä¸€ä¸ª DataFrameï¼Œè®¾ç½® index ä¸ºå…±åŒçš„ indexï¼Œcolumns ä¸ºå…±åŒçš„ columns
    average_df = pd.DataFrame(weighted_averages, index=common_index)

    return average_df


# =============================================================================
# df1 = pd.DataFrame({
#     'col1': [1, 2, 3],
#     'col2': [4, 5, 6]
# }, index=[10, 11, 12])
# 
# df2 = pd.DataFrame({
#     'col1': [7, np.nan, 9],  # ç¬¬äºŒè¡Œ col1 ä¸º NaN
#     'col3': [10, 11, 12]
# }, index=[10, 11, 12])
# 
# predict_dict = {('model1', 'test1'): df1, ('model2', 'test2'): df2}
# 
# # è°ƒç”¨å‡½æ•°è®¡ç®—åˆ—çš„å¹³å‡å€¼
# average_df = compute_dataframe_dict_average(predict_dict)
# 
# print(average_df)
# =============================================================================


# %%
def check_dataframe_consistency(df, new_data):
    """
    ä½¿ç”¨çŸ©é˜µè¿ç®—æ£€æŸ¥ä¸¤ä¸ªDataFrameåœ¨é‡å çš„ç´¢å¼•éƒ¨åˆ†å’Œåˆå¹¶åçš„åˆ—ä¸Šæ˜¯å¦å®Œå…¨ä¸€è‡´ã€‚
    å®Œå…¨ä¸€è‡´çš„å®šä¹‰:
    - ä¸¤ä¸ªå€¼éƒ½æ˜¯éNAä¸”ç›¸ç­‰
    - ä¸¤ä¸ªå€¼éƒ½æ˜¯NA
    - å¦‚æœä¸€ä¸ªå€¼æ˜¯NAè€Œå¦ä¸€ä¸ªä¸æ˜¯ï¼Œåˆ™è§†ä¸ºä¸ä¸€è‡´
    
    å‚æ•°:
    df (pd.DataFrame): ç›®æ ‡ DataFrameã€‚
    new_data (pd.DataFrame): è¦æ£€æŸ¥çš„æ–° DataFrameã€‚
    
    è¿”å›å€¼:
    tuple: (status, info)
        - status (str): 'CONSISTENT' è¡¨ç¤ºæ•°æ®ä¸€è‡´æˆ–æ²¡æœ‰é‡å ï¼›'INCONSISTENT' è¡¨ç¤ºå­˜åœ¨ä¸ä¸€è‡´
        - info (dict): å½“statusä¸º'INCONSISTENT'æ—¶ï¼ŒåŒ…å«ä¸ä¸€è‡´çš„è¯¦ç»†ä¿¡æ¯ï¼›å¦åˆ™ä¸ºç©ºå­—å…¸
    """
    # è·å–é‡å çš„ç´¢å¼•
    overlapping_indices = df.index.intersection(new_data.index)
    
    # å¦‚æœæ²¡æœ‰é‡å çš„ç´¢å¼•ï¼Œç›´æ¥è¿”å›ä¸€è‡´çŠ¶æ€
    if len(overlapping_indices) == 0:
        return "CONSISTENT", {}
    
    # è·å–è¦æ£€æŸ¥çš„åˆ—ï¼ˆä»…æ£€æŸ¥new_dataä¸­å­˜åœ¨çš„åˆ—ï¼‰
    columns_to_check = df.columns.intersection(new_data.columns)
    
    # å¦‚æœæ²¡æœ‰é‡å çš„åˆ—ï¼Œç›´æ¥è¿”å›ä¸€è‡´çŠ¶æ€
    if len(columns_to_check) == 0:
        return "CONSISTENT", {}
    
    # æå–é‡å éƒ¨åˆ†çš„æ•°æ®
    df_overlap = df.loc[overlapping_indices, columns_to_check]
    new_data_overlap = new_data.loc[overlapping_indices, columns_to_check]
    
    # æ£€æŸ¥NAçš„ä¸€è‡´æ€§
    df_is_na = df_overlap.isna()
    new_is_na = new_data_overlap.isna()
    
    # NAçŠ¶æ€åº”è¯¥ä¸€è‡´ï¼ˆéƒ½æ˜¯NAæˆ–éƒ½ä¸æ˜¯NAï¼‰
    na_inconsistent = (df_is_na != new_is_na)
    
    # æ£€æŸ¥éNAå€¼çš„ä¸€è‡´æ€§
    values_inconsistent = (df_overlap != new_data_overlap) & (~df_is_na) & (~new_is_na)
    
    # åˆå¹¶ä¸¤ç§ä¸ä¸€è‡´æƒ…å†µ
    inconsistent_mask = na_inconsistent | values_inconsistent
    
    # å¦‚æœæœ‰ä¸ä¸€è‡´çš„å…ƒç´ 
    if inconsistent_mask.any().any():
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸ä¸€è‡´çš„ä½ç½®
        inconsistent_positions = [(idx, col) for idx, col in zip(
            *np.where(inconsistent_mask.values)
        )]
        
        # è·å–ç¬¬ä¸€ä¸ªä¸ä¸€è‡´çš„ä½ç½®å’Œå€¼
        first_pos = inconsistent_positions[0]
        first_idx = overlapping_indices[first_pos[0]]
        first_col = columns_to_check[first_pos[1]]
        
        # è·å–ä¸ä¸€è‡´çš„å€¼
        df_value = df.loc[first_idx, first_col]
        new_value = new_data.loc[first_idx, first_col]
        
        # åˆ›å»ºè¯¦ç»†ä¿¡æ¯å­—å…¸
        info = {
            "index": first_idx,
            "column": first_col,
            "original_value": df_value,
            "new_value": new_value,
            "inconsistent_count": inconsistent_mask.sum().sum()
        }
        
        return "INCONSISTENT", info
    
    # å¦‚æœä»£ç æ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜æ‰€æœ‰é‡å éƒ¨åˆ†éƒ½æ˜¯ä¸€è‡´çš„
    return "CONSISTENT", {}


# %%
def expand_factor_data(factor_data, target_columns):
    """
    å¯¹è¾“å…¥çš„factor_dataè¿›è¡Œåˆ¤æ–­ï¼Œå¦‚æœæ˜¯Seriesæˆ–è€…åªæœ‰ä¸€åˆ—çš„DataFrameï¼Œ
    åˆ™å°†å…¶æ‰©å±•ä¸ºåŒ…å«target_columnsä¸­æŒ‡å®šçš„æ‰€æœ‰åˆ—ï¼Œæ¯åˆ—éƒ½å¡«å……åŸå§‹æ•°æ®çš„å€¼ã€‚
    
    å‚æ•°:
        factor_data: pandas Seriesæˆ–DataFrame, è¾“å…¥çš„å› å­æ•°æ®
        target_columns: list, ç›®æ ‡åˆ—ååˆ—è¡¨
        
    è¿”å›:
        pandas DataFrame, æ‰©å±•åçš„æ•°æ®æ¡†
    """
    # æ£€æŸ¥è¾“å…¥æ˜¯Seriesè¿˜æ˜¯DataFrame
    if isinstance(factor_data, pd.Series):
        # å¦‚æœæ˜¯Seriesï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„DataFrameï¼Œæ‰€æœ‰target_columnséƒ½å¡«å……ç›¸åŒçš„å€¼
        result = pd.DataFrame()
        for col in target_columns:
            result[col] = factor_data.copy()
        return result
    
    elif isinstance(factor_data, pd.DataFrame):
        # å¦‚æœæ˜¯DataFrameï¼Œæ£€æŸ¥æ˜¯å¦åªæœ‰ä¸€åˆ—
        if factor_data.shape[1] == 1:
            # åªæœ‰ä¸€åˆ—çš„DataFrameï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„DataFrameï¼Œæ‰€æœ‰target_columnséƒ½å¡«å……ç›¸åŒçš„å€¼
            original_column = factor_data.iloc[:, 0]  # è·å–ç¬¬ä¸€åˆ—æ•°æ®
            result = pd.DataFrame()
            for col in target_columns:
                result[col] = original_column.copy()
            return result
        else:
            # å¦‚æœDataFrameæœ‰å¤šäºä¸€åˆ—ï¼Œåˆ™è¿”å›åŸå§‹æ•°æ®ï¼Œæˆ–è€…æ ¹æ®éœ€æ±‚æŠ›å‡ºå¼‚å¸¸
            # è¿™é‡Œé€‰æ‹©è¿”å›åŸå§‹æ•°æ®ï¼Œä¹Ÿå¯ä»¥æ ¹æ®éœ€æ±‚ä¿®æ”¹ä¸ºæŠ›å‡ºå¼‚å¸¸
            return factor_data
    
    else:
        # å¦‚æœè¾“å…¥æ—¢ä¸æ˜¯Seriesä¹Ÿä¸æ˜¯DataFrameï¼ŒæŠ›å‡ºç±»å‹é”™è¯¯
        raise TypeError("è¾“å…¥çš„factor_dataå¿…é¡»æ˜¯pandas Seriesæˆ–DataFrameç±»å‹")
        
        
# %%
def deduplicate_nested_list(nested_list: list) -> list:
    """
    å¯¹åµŒå¥—åˆ—è¡¨è¿›è¡Œå»é‡
    
    å‚æ•°:
        nested_list (list): åŒ…å«å­åˆ—è¡¨çš„åµŒå¥—åˆ—è¡¨
        
    è¿”å›:
        list: å»é‡åçš„åµŒå¥—åˆ—è¡¨
    """
    # å°†æ¯ä¸ªå­åˆ—è¡¨è½¬æ¢ä¸ºå…ƒç»„ä»¥ä¾¿ä½¿ç”¨é›†åˆå»é‡
    unique_tuples = set(tuple(item) for item in nested_list)
    # è½¬å›åˆ—è¡¨æ ¼å¼
    return [list(item) for item in unique_tuples]


def _save_results(self, final_factors: pd.DataFrame, top_factors: pd.DataFrame, res_dir: Path, res_selected_test_dir: Path, save_to_all: bool = False) -> None:
    """
    ä¿å­˜ç­›é€‰ç»“æœå’Œå¤åˆ¶ç›¸å…³æ–‡ä»¶
    
    å‚æ•°:
        final_factors (pd.DataFrame): ç­›é€‰åçš„å› å­æ•°æ®
        top_factors (pd.DataFrame): é¡¶éƒ¨å› å­æ•°æ®ï¼ˆèšç±»å‰ï¼‰
        res_dir (Path): ç»“æœç›®å½•
        res_selected_test_dir (Path): é€‰å®šæµ‹è¯•çš„ç»“æœç›®å½•
        save_to_all (bool): æ˜¯å¦å°†ç»“æœä¿å­˜åˆ°ç±»å˜é‡ä¸­ï¼Œé»˜è®¤ä¸ºFalse
    """
    # ä¿å­˜å› å­åˆ—è¡¨ä¸ºJSON
    final_factors_to_list = list(zip(final_factors['root_dir'], 
                                 final_factors['process_name'], 
                                 final_factors['factor']))
    
    # å¯¹final_factors_to_listè¿›è¡Œå»é‡
    final_factors_to_list = deduplicate_nested_list(final_factors_to_list)
    
    # ä¿å­˜åˆ°ç»“æœç›®å½•
    with open(res_dir / 'final_factors.json', 'w', encoding='utf-8') as f:
        json.dump(final_factors_to_list, f, ensure_ascii=False, indent=4)
    
    # ä¿å­˜top_factorsåˆ°CSV
    top_factors.to_csv(res_dir / 'top_factors.csv', index=False)
    
    # å¦‚æœéœ€è¦ä¿å­˜åˆ°all_final_factors
    if save_to_all:
        # è·å–åŸå§‹å› å­åç§°
        org_fac_dir = res_dir.parent
        org_fac = org_fac_dir.name
        
        # å¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ç±»å˜é‡ï¼Œåˆ™åˆ›å»º
        if not hasattr(self, 'all_final_factors') or self.all_final_factors is None:
            self.all_final_factors = {}
        
        # å¦‚æœè¯¥åŸå§‹å› å­è¿˜æ²¡æœ‰åœ¨all_final_factorsä¸­ï¼Œåˆ™åˆ›å»º
        if org_fac not in self.all_final_factors:
            self.all_final_factors[org_fac] = []
        
        # å°è¯•è¯»å–åŸå§‹å› å­ç›®å½•ä¸­å·²æœ‰çš„å› å­åˆ—è¡¨
        all_factors_path = org_fac_dir / 'all_final_factors.json'
        existing_factors = []
        if all_factors_path.exists():
            try:
                with open(all_factors_path, 'r', encoding='utf-8') as f:
                    existing_factors = json.load(f)
            except:
                existing_factors = []
        
        # åˆå¹¶å¹¶å»é‡
        combined_factors = existing_factors + final_factors_to_list
        unique_factors = deduplicate_nested_list(combined_factors)
        
        # æ›´æ–°ç±»å˜é‡
        self.all_final_factors[org_fac] = unique_factors
        
        # ä¿å­˜åˆ°åŸå§‹å› å­ç›®å½•
        with open(all_factors_path, 'w', encoding='utf-8') as f:
            json.dump(unique_factors, f, ensure_ascii=False, indent=4)